RNN
RNN 쉘은 사장되는 중. 근데 LSTM등의 근간이 되기 때문에 알면 좋다.
순차데이터
    - 텍스트 ex) I am a boy
    - 시계열 ex) 1일 15도, 2일 17도, 3일 16도...
        시간 등의 연속적인 순서에 따라 데이터의 상관관계(증감)가 있는 데이터
    한번 전달이 아닌 연속적인 전달이 필요

    뉴런을 통과해 나온 결과(activation도 거치고)를 해당 뉴런에 새로운 입력과 함께 다시 통과한다.
        -순서를 기억하게 된 것.
        -그러나 자꾸 순환을 하며 값이 사라져버릴 수도 있다(activation을 거치며). 단기기억에 포커싱되어 있음.
    뉴런 자체가 순환해야하는 뉴런. 쉘. RNN뉴런.
    
    h activation function을 거친 값
    z activation function을 거치기 전의 결과 값

순환 신경망은 피드포워드 신경망과 매우 비슷하지만 뒤쪽으로 순환하는 연결도 있다는 점이 차이
벡터 투 시퀀스 네트워크: 각 타임 스텝에서 하나의 입력 벡터를 반복해서 네트워크에 주입하고 하나의 시퀀스를 출력
인코더 - 디코더

BPTT(backpropagation through time)
    RNN을 훈련하기 위한 기법은 타임 스텝으로 네트워크를 펼치고 보통의 역전파를 사용하는 전략
    정방향 패스는 다섯개의 입력 시퀀스가 주입된 다섯 번의 타임스텝을 말함
    순환 신경망은 일련의 타임 스텝을 진행하고 나서 그레디언트가 전파된다.

시계열: 타임스텝마다 하나 이상의 값을 가진 시퀀스
    단변량 시계열: 타임스텝이 하나. 웹사이트에서 시간당 접속 사용자의 수, 도시의 날자별 온도 등.
    다변량 시계열: 기업의 분기별 재정 안정성 지표(회사의 수입, 부채 등)처럼 타임스텝마다 여러 값이 존재.

텍스트 - 문장을 인식하기 위해 단어의 순서가 필요하고 단어를 인식하기 위해 숫자화가 필요.
    단어마다의 데이터를 연속적으로 넣어야 하므로 단어의 수만큼 타임스텝이 결정.(?)

RNN의 문제
    불안정한 그레디언트 문제
    학습이 지속되면 이전 기억의 소실
    과거 학습요소 상실
    단기 기억 문제

단기기억 문제 해결하기
    LSTM(장단기 메모리)셀
    핍홀 연결
    GRU(게이트 순환 유닛)셀
    1D 합성곱 층을 사용해 시퀀스 처리하기
    WAVENET


텍스트 전처리

말뭉치(코퍼스)
    말뭉치 또는 코퍼스(corpus, corpora)
    자연언어 연구를 위해 특정한 목적을 가지고 언어의 표본을 추출한 집합
    자연어 처리 관련 애플리케이션은 방대한 양의 데이터
    코퍼스 분석 뿐만 아니라 언어 분석에서 사용되는 실제 언어의 체계적 디지털 모음
    둘 이상의 코퍼스가 있으면 코포라라고 부름
    코퍼스를 데이터세트라고도 함

    언어를 디지털화 한 것이 코퍼스

    단일 언어 코퍼스: 하나의 언어로 이루어짐
    이중 언어 코퍼스: 2개의 언어로 이루어짐
    다국어 코퍼스: 3개 이상의 언어로 이루어짐

텍스트 전처리
    자연어 처리에서 크롤링 등으로 얻어낸 코퍼스 데이터가 필요에 맞게 전처리되지 않은 상태시
    해당 데이터를 용도에 맞게 토큰화 & 정제 & 정규화를 진행해야함
    
    -토큰화: 용도에 맞게 자르는 것
        단어 토큰화
            토큰의 기준을 단어로 하여 토큰화하는 것
            단어는 단어 외에도 단어구, 의미를 갖는 문자열로도 간주됨
            보통 토큰화 작업은 단순히 구두점이나 특수 문자를 전부 제거하는 정제 작업을 수행하는 것만으로 해결되지 않음
            구두점이나 특수문자를 전부 제거하면 토큰이 의미를 잃어버리는 경우가 발생함
            띄어쓰기 단위로 자를시 단어 토큰 구분이 망가지는 언어도 존재
            단어 토큰화의 목적은 vocabulary를 만드는 것. 

        문장 토큰화
            문장 단위로 구분하는 장겁으로 때로는 문장 분류(sentence segmentation)
            정제되지 않은 상태라면 코퍼스는 문장 단위로 구분되어 있지 않아서 이를 사용하고자 하는 용도에 맞게 문장 토큰화가 필요
            느낌표나 물음표는 문장의 구분을 위한 꽤 명확한 구분자 역할
            마침표는 문장의 끝이 아니더라도 등장할 수 있다.

        한국어 토큰화
            영어는 New York과 같은 합성어나 he's와 같이 줄임말에 대한 예외처리만 한다면 띄어쓰기를 기준으로 하는 띄어쓰기 토큰화를 수행해도 단어 토큰화가 잘 작동
            한국어는 영어와는 달리 띄어쓰기만으로는 토큰화를 하기에 부족하다.
            한국어의 띄어쓰기 단위는 어절이라 하는데 어절 토큰화는 한국어 NLP에서 지양되고 있다.
            한국어는 교착어(조사, 어미 등을 붙인 언어)
            한국어는 어절이 독립적인 단어로 구성되는 것이 아니라 이를 전부 분리해줘야 한다.

            형태소(자립, 의존, 실질, 형식)를 이해해야 한다.

            한국어는 띄어쓰기가 영어보다 잘 지켜지지 않는다.

            단어는 표기는 같지만 품사에 따라 단어의 의미가 달라지기도 함

            품사 태깅
                단어 토큰화 과정에서 각 단어가 어떤 품사로 쓰였는지를 구분해놓는 작업


        토큰화의 기준을 정할 때는 신중히.
        일단 그 언어를 잘 알아야 한다.

        고려사항:
            구두점, 특수문자 등을 단순 제외해선 안된다.
                구두점조차도 하나의 토큰으로 분류하기도 한다.
                ex) 마침표(.) - 문장의 경계
                    단어 자체에 구두점 - m.p.h, Ph.D, AT&T, $1000 등
            줄임말과 단어 내에 띄어쓰기가 있는 경우
                ex) he's, New York
        penn treebank
    
    -정제: 컴퓨터가 알아먹도록 하는 것
        갖고 있는 코퍼스로부터 노이즈 데이터를 제거
        완벽한 정제 작업은 어려운 편
        대부분의 경우 이 정도면 됐다는 일종의 합의점을 찾음

    -정규화: 정제된 값을 정규화
        표현 방법이 다른 단어들을 통합시켜서 같은 단어로 만들기

        규칙에 기반한 통합
            정규화 규칙의 예로서 같은 의미를 갖고 있음에도 표기가 다른 단어들을 하나의 단어로 정규화하는 방법을 사용할 수 있음
        대소문자 통합
            대부분의 글은 소문자로 작성되기 때문에 대소문자 통합 작업은 대부분 대문자를 소문자로 변환하는 소문자 변환 작업
        불필요한 단어의 제거
            등장 빈도가 적은 단어
            길이가 짧은 단어(의미가 너무 많이 담겨있거나 추임새인 경우)
        정규화 기법 중 단어의 개수를 줄일 수 있는 기법
            표제어 추출
                기본 사전형 단어 정도의 의미
                표제어 추출은 단어들이 다른 형태를 가지더라도 그 뿌리 단어를 찾아가서 단어의 개수를 줄일 수 있는지 판단
            어간 추출
                형태학적 분석을 단순화한 버전
        하나의 단어로 일반화시킬 수 있다면 하나의 단어로 일반화시켜서 문서 내의 단어 수를 줄이겠다는 것
        자연어 처리에서 전처리 정규화의 지향점은 언제나 갖고 있는 복잡성을 줄이려는 것
    
    불용어(stopword)
        실제 의미 분석을 하는데 거의 기여한느 바가 없는 단어들
        한국어에서 불용어를 제거하는 방법으로 간단하게는 토큰화 후에 조사, 접속사 등을 제거
        사용자가 직접 불용어 사전을 만들게 되는 경우가 많다.
        불용어가 많은 경우에는 코드 내에서 직접 정의하지 않고 txt파일이나 csv파일로 정리해놓고 이를 불러와서 사용하기도 함.
        한국어에선 접사, 어미, 조사 등 형식형태소인 단어를 없애곤 한다.

    정규표현식.
        문자라는 단어를 표현하기 위한 수식, 방법 
        . 한개의 임의의 문자를 나타낸다.(줄바꿈 문자인 \n는 제외)
        ? 앞의 문자가 존재할 수도 있고 존재하지 않을 수도 있다.(문자가 0개 또는 1개)
        * 앞의 문자가 무한개로 존재할 수도 있고 존재하지 않을 수도 있다.(문자가 0개 이상)
        + 앞의 문자가 최소 한 개 이상 존재한다.
        ^ 뒤의 문자열로 문자열이 시작된다.
        $ 앞의 문자열로 문자열이 끝난다
        {숫자} 숫자만큼 반복
        {숫자1, 숫자2} 숫자1 이상 숫자2 이하만큼 반복. ?,*,+를 이것으로 대체할 수 있다.
        {숫자,} 숫자 이상만큼 반복
        [^문자] 해당 문자를 제외한 문자를 매치한다.

*자연어 처리는 사람의 판단이 많이 들어가므로 왜곡되고 편향된 데이터가 될 수도 있다.
*텍스트 전처리는 항상 순차적으로 진행되는 건 아님.(사실 일반 데이터 또한 그렇긴 하다.)
*시계열 데이터인지 이미지 데이터인지 정하는 것은 전처리에 따라 결정된다.

    정수 인코딩
        각 단어를 고유한 정수에 맵핑시키는 전처리 작업
        단어를 빈도수 순으로 정렬한 단어 집합을 만드고 빈도수가 높은 순서대로 번호를 매긴다.
    패딩
        문장의 단어 수를 맞춰주는 작업
    
    원핫 인코딩
        단어 집합의 크기를 벡터의 차원으로 하고, 표현하고 싶은 단어의 인덱스에 1의 값을 부여하고 다른 인덱스에는 0을 부여하는 단어의 벡터 표현 방식
        데이터의 차원화.
    단어집합(vocabulary)
        기본적으로 book과 books와 같이 단어의 변형 형태도 다른 단어로 간주
    원핫 인코딩의 한계
        벡터를 저장하기 우해 필요한 공간이 계속 늘어난다. 데이터를 잃어버릴 수 있음
        다른 표현으로는 벡터의 차원이 늘어난다고 표현
    
    언어모델(LM)
        단어 시퀀스에 확률을 할당하는 모델, 이전 단어들을 이용하여 다음 단어를 예측함
        BERT는 양쪽 단어들로부터 가운데 단어 예측
        
        통계를 이용한 방법과 인공신경망을 이용한 방법
        최근에는 인공신경망 방법이 더 성능 좋음

        통계적 언어 모델(Statistical Language Model, SLM)
            확률 기반의 언어모델
            m개의 단어 w1,w2,...,wm열(word sequence)이 주어졌을 때 문장으로써 성립될 확률P(w1,w2,..,wm)을 출력함으로써
                이 단어 열이 실제로 현실에서 사용될 수 있는 문장인지를 판별하는 모델
            조건부 확률
                유니그램
                    모든 단어의 활용이 완전히 서로 독립이다.
                바이그램
                    단어의 활용이 바로 전 단어와만 관련이 있다.
                N그램 
                    여러개의 단어가 관련을 짓고 있다.
                
                *바이그램, n그램은 첫 단어와 끝 단어에 가중치가 붙는다.(처음과 끝 단어를 알려주기 위해)
                
            문장에 대한 확률을 카운트 기반의 접근으로 계산
            SLM의 한계
                훈련 코퍼스에 확률을 계산하고 싶은 문장이나 단어가 없을 수 있다는 점
                확률을 계산하고 싶은 문장이 길어질수록 갖고 있는 코퍼스에서 그 문장이 존재하지 않을 가능성이 높다.
                학습에 관해 말할 때 조건에 맞는 코퍼스가 없을 수 있다..

            N-gram 언어 모델
                임의의 개수를 정하기 위한 기준을 위해 사용하는 것
                n개의 연속적인 단어 나열
                n개의 단어 뭉치 단위로 끊어서 이를 하나의 토큰으로 간주
                n-gram을 통한 언어 모델에서는 다음에 나올 단어의 예측은 오직 n-1개의 단어에만 의존
                한 단어씩 지나며 묶는다. ex) i am a boy -> i am a, am a boy
                한계
                    희소문제 - 충분한 데이터를 관측하지 못해 언어를 정확히 모델링하지 못하는 문제
                    n을 선택하는 것은 trade-off 문제
    
    단어표현
        국소표현(이산 표현, BoW, DTM, TF-IDF)
            Bag of Words
                단어들의 순서는 전혀 고려하지 않고 단어들의 출현 빈도에 초점을 맞춘 텍스트 데이터 수치화 표현 방법
            문서 단어 행렬(Document-Term Matrix)
                BoW들을 결합한 표현 방법
                빈도를 행렬로 표현
                희소문제가 발생한다.
            TF-IDF(Term Frequency - Inverse Document Frequency)
                쉽게 말해 문서에 대한 특정 단어의 빈도 수, 특정 문장에 대해 특정 단어의 빈도 수라고 생각하면 될 것도 같다.
                단어의 빈도와 역 문서 빈도(문서의 빈도에 특정 식을 취함)를 사용하여 DTM 내의 각 단어들마다 중요한 정도를 가중치로 주는 방법
                주로 문서의 유사도를 구하는 작업, 검색 시스템에서 검색 결과의 중요도를 정하는 작업, 문서 내에서 특정 단어의 중요도를 구하는 작업 등 사용



        분산표현
            연속 표현
            Word2Vec -임베딩. 공간에 뿌려서 벡터로 만드는 것

벡터의 유사도
    문장이나 문서의 유사도
        문서의 유사도는 주로 문서들 간에 동일한 단어 또는 비슷한 단어가 얼마나 공통적으로 많이 사용되었는지에 의존
    
    벡터의 유사도
        코사인 유사도
            두 벡터 간의 코사인 각도를 이용하여 구할 수 있는 두 벡터의 유사도. 벡터의 방향성을 비교한다.
            방향성을 따져 유사도가 1에 가까울수록 같은 선상에 있다. 
            유사도가 -1에 가까울수록 다른 선상. 방향이 다름
        자카드 유사도
            합집합에서 교집합의 비율을 바탕으로 대상의 유사도 계산방식
            0과 1 사이의 값. 동일하다면 1, 공통원소가 없다면 0
        유클리디안 유사도
            피타고라스의 정리를 통해 두 점 사이의 거리를 구하는 것과 동일
            점과 점 사이의 거리
        멘하탄 유사도
            격자로 이루어진 공간에서 출발점에서 도착점까지 길을 따라 만들어진 최단거리
        
        코사인 유사도가 가장 널리 쓰인다. 단순 좌표상의 거리가 아닌 단어의 공간에서 벡터 간의 각도를 구하기 때문에.

LSTM
    RNN의 결정적 단점인 단기기억을 보완
    망각 게이트는 과거 정보를 어느정도 기억할지 결정
        과거 정보와 현재 데이터를 입력받아 시고모이드를 취한 후 그 값을 과거 정보에 곱해줌
        시그모이드 출력이 0이면 과거 정보는 버리고 1이면 과거 정보는 보존

    입력게이트
        현재 정보를 기억하기 위해 만들어졌음
        과거 정보와 현재 데이터를 입력받아 시그모이드와 하이퍼볼릭 탄젠트 함수를 기반ㅇ으로 현재 정보에 대한 보존량을 결정
        즉 현재 메모리에 새로운 정보를 반영할지 결정하는 역할을 함
        계산한 값이 1이면 입력 xt가 들어올 수 있도록 허용
        계산한 값이 0이면 차단
    
    셀
        각 단계에 대한 은닉 노드를 메모리 셀이라고 함
        총합을 사용하여 셀 값을 반영하며 이것으로 기울기 소멸 문제가 해결
        망각게이트와 입력게이트의 이전 단계 셀 정보를 

GRU
    게이트 메커니즘이 적용된 RNN프레임워크의 한 종류이면서 LSTM보다 구조가 간단함
    GRU는 LSTM에서 사용하는 망각 게이트와 입력 게이트를 하나로 합친 것이며, 별도의 업데이트 게이트로 구성
    하나의 게이트 컨트롤러가 망각 게이트와 입력 게이트를 모두 제어함
    게이트 컨트롤러가 1일 출력하면 망각 게이트는 열리고 입력 게이트는 닫히며 반대로 0을 출력하면 망각게이트는 닫히고 입력게이트는 열린다.

양방향RNN
    RNN은 이전 시점의 데이터들을 참고해서 정답을 예측하지만 실제 문제에서는 과거 ㅣ점이 아닌 미래 시점의 데이터에 힌트가 있는 경우도 많음
    